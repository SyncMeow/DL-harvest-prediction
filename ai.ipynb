{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xuQHrmwynpkQ",
        "outputId": "7f40dd95-7421-45f1-d8df-6335fe491ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.0+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class model(nn.Module):\n",
        "    def __init__(self, input_dim=22, output_dim=1, num_hidden_layers=20, hidden_dim=100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        for i in range(num_hidden_layers):\n",
        "            self.hidden_layers.append(nn.Linear(input_dim if i == 0 else hidden_dim, hidden_dim))\n",
        "\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for idx, layer in enumerate(self.hidden_layers):\n",
        "            if (idx+1 % 5):\n",
        "                x = self.dropout(x)\n",
        "            x = torch.relu(layer(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "It1SC0kgJDf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func1(x:list):\n",
        "    w = [1, 1, 2, 2, 2, 4, 4, 5, 5, 8, 9, 10, 11, 11, 10, 7, 5, 3, 3, 2, 2, 1]\n",
        "    ans = 0\n",
        "    for i in range(len(w)):\n",
        "        ans += w[i]*x[i]/sum(w)\n",
        "    return ans\n",
        "\n",
        "def func2(x:list):\n",
        "    ans = 0\n",
        "    for i in x:\n",
        "        ans += pow(0.91, 0.04*max(0,i-750)) * pow(0.87, 0.01*max(0,750-i)) / len(x)\n",
        "    return ans\n",
        "\n",
        "def func3(x:list):\n",
        "    ans = 0\n",
        "    for i in x:\n",
        "        ans += pow(0.91, 0.07*max(0,i-200)) * pow(0.93, 0.01*max(0,200-i)) / len(x)\n",
        "    return ans\n",
        "\n",
        "def PredictH(x:list):\n",
        "    ans = (func3(x[0:8])+func3(x[10:20]))*func2(x[9:17])*func1(x)\n",
        "    return ans"
      ],
      "metadata": {
        "id": "VeVINf6QVpHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class data_set(Dataset):\n",
        "    def __init__(self, data, label):\n",
        "        data = torch.tensor(data).float()\n",
        "        data = F.normalize(data)\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.tensor(self.data[index]).float(), torch.tensor(self.label[index]).float()"
      ],
      "metadata": {
        "id": "8ii6-Op6eTk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(dtran, size):\n",
        "    a, b, c, d, e, f = int(dtran[0]), int(dtran[1]), int(dtran[2]), int(dtran[3]), int(dtran[4]), int(dtran[5])\n",
        "    DATA = [[np.random.randint(a, b) for i in range(7)] + [np.random.randint(c, d) for i in range(8)] + [np.random.randint(e, f) for i in range(7)] for i in range(size)]\n",
        "    for i in range(len(DATA)):\n",
        "        for j in range(i+1, len(DATA)):\n",
        "            if (DATA[i] == DATA[j]):\n",
        "                DATA[j] = [0 for k in range(22)]\n",
        "    LABEL = []\n",
        "    for data in DATA:\n",
        "        LABEL.append([PredictH(data)])\n",
        "    return DATA, LABEL"
      ],
      "metadata": {
        "id": "x2IrvXHWekD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def fetch_data():\n",
        "    PATH = \"\"\n",
        "    with open(PATH, \"r\", encoding = \"utf-8\") as r:\n",
        "        jdata = json.load(r)\n",
        "\n",
        "    DATA = []\n",
        "    LABEL = []\n",
        "    for value in jdata[\"data\"]:\n",
        "        DATA.append(value[0])\n",
        "        LABEL.append(value[1])   \n",
        "    return DATA, LABEL"
      ],
      "metadata": {
        "id": "gf_Rq6eskNX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "batch_size = 24\n",
        "n_epoch = 150\n",
        "\n",
        "DATA, LABEL = generate_data([0, 400, 250, 1500, 0, 500], 10000)\n",
        "print(len(DATA))\n",
        "datasplit = int(len(DATA)/5)\n",
        "DATA1, LABEL1 = DATA[datasplit:], LABEL[datasplit:]\n",
        "DATA2, LABEL2 = DATA[:datasplit], LABEL[:datasplit]\n",
        "\n",
        "print(len(DATA2))\n",
        "\n",
        "train_dataset = data_set(DATA1, LABEL1)\n",
        "test_dataset = data_set(DATA2, LABEL2)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = model()\n",
        "model.to(device)\n",
        "learning_rate = 0.001\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def fit(epoch, model, dataloader, training = True):\n",
        "    if training: \n",
        "        model.train() \n",
        "    else: \n",
        "        model.eval()\n",
        "\n",
        "    mode = \"training\" if training else \"validation\"\n",
        "    running_loss = 0.0\n",
        "\n",
        "    cnt = 0\n",
        "    for index, (data, target) in enumerate(dataloader):\n",
        "        cnt += 1\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if (training):\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    #loss = running_loss / len(dataloader.dataset)\n",
        "    loss = running_loss / cnt\n",
        "    print(f'<Epoch {epoch}> {mode} loss: {round(loss, 4)}')\n",
        "    return loss\n",
        "\n",
        "#training\n",
        "train_losses, val_losses = [], []\n",
        "\n",
        "for epoch in range(1, n_epoch+1):\n",
        "    epoch_train_loss = fit(epoch, model, train_dataloader, True)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    epoch_val_loss = fit(epoch, model, test_dataloader, False)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "torch.save(model.state_dict(), './model.ckpt')\n",
        "print(\"Model saved\")\n",
        "\n",
        "#visualize\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'bo', label = 'training loss')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 'r', label = 'validation loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dYG-FfVugxXs",
        "outputId": "a5ff4d55-27f2-4ea0-ee40-98bc2242a28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-255e1971e204>:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(self.data[index]).float(), torch.tensor(self.label[index]).float()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Epoch 1> training loss: 19586.5752\n",
            "<Epoch 1> validation loss: 4851.0678\n",
            "<Epoch 2> training loss: 5122.047\n",
            "<Epoch 2> validation loss: 2202.9741\n",
            "<Epoch 3> training loss: 4240.279\n",
            "<Epoch 3> validation loss: 2148.4988\n",
            "<Epoch 4> training loss: 3914.2704\n",
            "<Epoch 4> validation loss: 2281.2791\n",
            "<Epoch 5> training loss: 3587.7332\n",
            "<Epoch 5> validation loss: 2457.0051\n",
            "<Epoch 6> training loss: 3488.6503\n",
            "<Epoch 6> validation loss: 2462.1047\n",
            "<Epoch 7> training loss: 3387.3513\n",
            "<Epoch 7> validation loss: 2075.1522\n",
            "<Epoch 8> training loss: 3174.1222\n",
            "<Epoch 8> validation loss: 2111.5049\n",
            "<Epoch 9> training loss: 3131.8753\n",
            "<Epoch 9> validation loss: 2095.0149\n",
            "<Epoch 10> training loss: 3092.229\n",
            "<Epoch 10> validation loss: 2188.3608\n",
            "<Epoch 11> training loss: 2984.1062\n",
            "<Epoch 11> validation loss: 2198.63\n",
            "<Epoch 12> training loss: 2944.9408\n",
            "<Epoch 12> validation loss: 2068.8711\n",
            "<Epoch 13> training loss: 2945.305\n",
            "<Epoch 13> validation loss: 2071.9619\n",
            "<Epoch 14> training loss: 2904.1808\n",
            "<Epoch 14> validation loss: 2411.4395\n",
            "<Epoch 15> training loss: 2885.6173\n",
            "<Epoch 15> validation loss: 2223.5467\n",
            "<Epoch 16> training loss: 2835.661\n",
            "<Epoch 16> validation loss: 2268.6166\n",
            "<Epoch 17> training loss: 2820.9784\n",
            "<Epoch 17> validation loss: 2089.6268\n",
            "<Epoch 18> training loss: 2788.6562\n",
            "<Epoch 18> validation loss: 2104.5259\n",
            "<Epoch 19> training loss: 2820.7098\n",
            "<Epoch 19> validation loss: 2112.9677\n",
            "<Epoch 20> training loss: 2753.5987\n",
            "<Epoch 20> validation loss: 2093.6461\n",
            "<Epoch 21> training loss: 2773.0609\n",
            "<Epoch 21> validation loss: 2220.9333\n",
            "<Epoch 22> training loss: 2732.6374\n",
            "<Epoch 22> validation loss: 2095.9079\n",
            "<Epoch 23> training loss: 2784.9188\n",
            "<Epoch 23> validation loss: 2334.2046\n",
            "<Epoch 24> training loss: 2693.0947\n",
            "<Epoch 24> validation loss: 2076.9269\n",
            "<Epoch 25> training loss: 2698.8656\n",
            "<Epoch 25> validation loss: 2102.7156\n",
            "<Epoch 26> training loss: 2685.3574\n",
            "<Epoch 26> validation loss: 2079.6533\n",
            "<Epoch 27> training loss: 2572.0192\n",
            "<Epoch 27> validation loss: 2084.2973\n",
            "<Epoch 28> training loss: 2631.4788\n",
            "<Epoch 28> validation loss: 2069.5266\n",
            "<Epoch 29> training loss: 2592.2972\n",
            "<Epoch 29> validation loss: 2060.9451\n",
            "<Epoch 30> training loss: 2650.7983\n",
            "<Epoch 30> validation loss: 2086.7617\n",
            "<Epoch 31> training loss: 2637.2609\n",
            "<Epoch 31> validation loss: 2067.6279\n",
            "<Epoch 32> training loss: 2561.8081\n",
            "<Epoch 32> validation loss: 2240.2707\n",
            "<Epoch 33> training loss: 2611.5067\n",
            "<Epoch 33> validation loss: 2066.396\n",
            "<Epoch 34> training loss: 2550.3322\n",
            "<Epoch 34> validation loss: 2102.6587\n",
            "<Epoch 35> training loss: 2578.9874\n",
            "<Epoch 35> validation loss: 2193.6046\n",
            "<Epoch 36> training loss: 2562.7739\n",
            "<Epoch 36> validation loss: 2141.3182\n",
            "<Epoch 37> training loss: 2559.2103\n",
            "<Epoch 37> validation loss: 2090.4891\n",
            "<Epoch 38> training loss: 2551.6\n",
            "<Epoch 38> validation loss: 2079.1391\n",
            "<Epoch 39> training loss: 2551.9399\n",
            "<Epoch 39> validation loss: 2070.3474\n",
            "<Epoch 40> training loss: 2553.5021\n",
            "<Epoch 40> validation loss: 2080.4431\n",
            "<Epoch 41> training loss: 2549.2878\n",
            "<Epoch 41> validation loss: 2076.6426\n",
            "<Epoch 42> training loss: 2552.2584\n",
            "<Epoch 42> validation loss: 2273.2371\n",
            "<Epoch 43> training loss: 2497.9258\n",
            "<Epoch 43> validation loss: 2106.3504\n",
            "<Epoch 44> training loss: 2519.6088\n",
            "<Epoch 44> validation loss: 2087.1168\n",
            "<Epoch 45> training loss: 2494.0957\n",
            "<Epoch 45> validation loss: 2076.8869\n",
            "<Epoch 46> training loss: 2496.8588\n",
            "<Epoch 46> validation loss: 2088.9917\n",
            "<Epoch 47> training loss: 2465.0715\n",
            "<Epoch 47> validation loss: 2215.1872\n",
            "<Epoch 48> training loss: 2475.3924\n",
            "<Epoch 48> validation loss: 2237.5647\n",
            "<Epoch 49> training loss: 2462.6856\n",
            "<Epoch 49> validation loss: 2221.123\n",
            "<Epoch 50> training loss: 2459.3263\n",
            "<Epoch 50> validation loss: 2060.8564\n",
            "<Epoch 51> training loss: 2441.1802\n",
            "<Epoch 51> validation loss: 2150.4921\n",
            "<Epoch 52> training loss: 2422.6636\n",
            "<Epoch 52> validation loss: 2082.417\n",
            "<Epoch 53> training loss: 2420.5233\n",
            "<Epoch 53> validation loss: 2098.2137\n",
            "<Epoch 54> training loss: 2465.8367\n",
            "<Epoch 54> validation loss: 2202.0382\n",
            "<Epoch 55> training loss: 2420.0675\n",
            "<Epoch 55> validation loss: 2312.718\n",
            "<Epoch 56> training loss: 2412.7419\n",
            "<Epoch 56> validation loss: 2071.0721\n",
            "<Epoch 57> training loss: 2402.1879\n",
            "<Epoch 57> validation loss: 2183.076\n",
            "<Epoch 58> training loss: 2377.6233\n",
            "<Epoch 58> validation loss: 2111.5314\n",
            "<Epoch 59> training loss: 2359.1835\n",
            "<Epoch 59> validation loss: 2113.8067\n",
            "<Epoch 60> training loss: 2368.7256\n",
            "<Epoch 60> validation loss: 2696.9327\n",
            "<Epoch 61> training loss: 2339.0656\n",
            "<Epoch 61> validation loss: 2259.4706\n",
            "<Epoch 62> training loss: 2385.8257\n",
            "<Epoch 62> validation loss: 3174.4036\n",
            "<Epoch 63> training loss: 2346.1036\n",
            "<Epoch 63> validation loss: 2238.2398\n",
            "<Epoch 64> training loss: 2319.7586\n",
            "<Epoch 64> validation loss: 2399.9199\n",
            "<Epoch 65> training loss: 2313.3104\n",
            "<Epoch 65> validation loss: 2266.1464\n",
            "<Epoch 66> training loss: 2319.5898\n",
            "<Epoch 66> validation loss: 2542.032\n",
            "<Epoch 67> training loss: 2325.9489\n",
            "<Epoch 67> validation loss: 2158.3959\n",
            "<Epoch 68> training loss: 2303.5735\n",
            "<Epoch 68> validation loss: 2651.4826\n",
            "<Epoch 69> training loss: 2321.1418\n",
            "<Epoch 69> validation loss: 2582.0744\n",
            "<Epoch 70> training loss: 2288.4953\n",
            "<Epoch 70> validation loss: 2830.824\n",
            "<Epoch 71> training loss: 2276.0887\n",
            "<Epoch 71> validation loss: 2424.3129\n",
            "<Epoch 72> training loss: 2274.4407\n",
            "<Epoch 72> validation loss: 2433.2454\n",
            "<Epoch 73> training loss: 2283.0344\n",
            "<Epoch 73> validation loss: 2870.8316\n",
            "<Epoch 74> training loss: 2291.3229\n",
            "<Epoch 74> validation loss: 3256.2943\n",
            "<Epoch 75> training loss: 2290.5129\n",
            "<Epoch 75> validation loss: 2581.47\n",
            "<Epoch 76> training loss: 2283.1339\n",
            "<Epoch 76> validation loss: 2502.7686\n",
            "<Epoch 77> training loss: 2310.2803\n",
            "<Epoch 77> validation loss: 2773.6048\n",
            "<Epoch 78> training loss: 2278.7514\n",
            "<Epoch 78> validation loss: 2786.9025\n",
            "<Epoch 79> training loss: 2267.0429\n",
            "<Epoch 79> validation loss: 3155.8588\n",
            "<Epoch 80> training loss: 2283.6421\n",
            "<Epoch 80> validation loss: 2829.7014\n",
            "<Epoch 81> training loss: 2278.9432\n",
            "<Epoch 81> validation loss: 2471.0938\n",
            "<Epoch 82> training loss: 2277.6098\n",
            "<Epoch 82> validation loss: 2807.791\n",
            "<Epoch 83> training loss: 2258.7816\n",
            "<Epoch 83> validation loss: 3129.8387\n",
            "<Epoch 84> training loss: 2248.9829\n",
            "<Epoch 84> validation loss: 2661.6848\n",
            "<Epoch 85> training loss: 2284.5187\n",
            "<Epoch 85> validation loss: 2540.0322\n",
            "<Epoch 86> training loss: 2262.5413\n",
            "<Epoch 86> validation loss: 2842.1664\n",
            "<Epoch 87> training loss: 2252.3689\n",
            "<Epoch 87> validation loss: 3422.0181\n",
            "<Epoch 88> training loss: 2280.4728\n",
            "<Epoch 88> validation loss: 2680.1603\n",
            "<Epoch 89> training loss: 2247.0286\n",
            "<Epoch 89> validation loss: 2998.3674\n",
            "<Epoch 90> training loss: 2264.785\n",
            "<Epoch 90> validation loss: 2828.6978\n",
            "<Epoch 91> training loss: 2276.3217\n",
            "<Epoch 91> validation loss: 2682.0425\n",
            "<Epoch 92> training loss: 2267.0581\n",
            "<Epoch 92> validation loss: 2766.0016\n",
            "<Epoch 93> training loss: 2285.1338\n",
            "<Epoch 93> validation loss: 3479.5617\n",
            "<Epoch 94> training loss: 2260.996\n",
            "<Epoch 94> validation loss: 2808.9816\n",
            "<Epoch 95> training loss: 2282.3958\n",
            "<Epoch 95> validation loss: 3104.2786\n",
            "<Epoch 96> training loss: 2262.5673\n",
            "<Epoch 96> validation loss: 3424.5637\n",
            "<Epoch 97> training loss: 2267.6677\n",
            "<Epoch 97> validation loss: 3536.7231\n",
            "<Epoch 98> training loss: 2269.3345\n",
            "<Epoch 98> validation loss: 3464.601\n",
            "<Epoch 99> training loss: 2261.028\n",
            "<Epoch 99> validation loss: 2627.1756\n",
            "<Epoch 100> training loss: 2238.3555\n",
            "<Epoch 100> validation loss: 2909.9857\n",
            "<Epoch 101> training loss: 2260.4079\n",
            "<Epoch 101> validation loss: 2512.1296\n",
            "<Epoch 102> training loss: 2242.524\n",
            "<Epoch 102> validation loss: 3352.9079\n",
            "<Epoch 103> training loss: 2232.4131\n",
            "<Epoch 103> validation loss: 2916.003\n",
            "<Epoch 104> training loss: 2251.8825\n",
            "<Epoch 104> validation loss: 3052.0497\n",
            "<Epoch 105> training loss: 2244.0357\n",
            "<Epoch 105> validation loss: 2973.8308\n",
            "<Epoch 106> training loss: 2231.4682\n",
            "<Epoch 106> validation loss: 2429.851\n",
            "<Epoch 107> training loss: 2250.8718\n",
            "<Epoch 107> validation loss: 2532.3298\n",
            "<Epoch 108> training loss: 2247.5246\n",
            "<Epoch 108> validation loss: 3105.7803\n",
            "<Epoch 109> training loss: 2242.6874\n",
            "<Epoch 109> validation loss: 3126.6441\n",
            "<Epoch 110> training loss: 2225.6518\n",
            "<Epoch 110> validation loss: 3556.9448\n",
            "<Epoch 111> training loss: 2244.9515\n",
            "<Epoch 111> validation loss: 2513.9729\n",
            "<Epoch 112> training loss: 2226.8573\n",
            "<Epoch 112> validation loss: 3194.0763\n",
            "<Epoch 113> training loss: 2254.1903\n",
            "<Epoch 113> validation loss: 2914.1858\n",
            "<Epoch 114> training loss: 2247.1111\n",
            "<Epoch 114> validation loss: 2877.2419\n",
            "<Epoch 115> training loss: 2225.4698\n",
            "<Epoch 115> validation loss: 2719.9405\n",
            "<Epoch 116> training loss: 2240.395\n",
            "<Epoch 116> validation loss: 3580.263\n",
            "<Epoch 117> training loss: 2215.9768\n",
            "<Epoch 117> validation loss: 2927.2493\n",
            "<Epoch 118> training loss: 2252.821\n",
            "<Epoch 118> validation loss: 2622.0559\n",
            "<Epoch 119> training loss: 2221.9227\n",
            "<Epoch 119> validation loss: 2974.8438\n",
            "<Epoch 120> training loss: 2208.2033\n",
            "<Epoch 120> validation loss: 2386.1275\n",
            "<Epoch 121> training loss: 2241.6213\n",
            "<Epoch 121> validation loss: 3005.072\n",
            "<Epoch 122> training loss: 2226.2102\n",
            "<Epoch 122> validation loss: 2629.4273\n",
            "<Epoch 123> training loss: 2232.3361\n",
            "<Epoch 123> validation loss: 2474.7932\n",
            "<Epoch 124> training loss: 2227.9945\n",
            "<Epoch 124> validation loss: 2392.5156\n",
            "<Epoch 125> training loss: 2238.4237\n",
            "<Epoch 125> validation loss: 2980.4594\n",
            "<Epoch 126> training loss: 2238.29\n",
            "<Epoch 126> validation loss: 2545.0932\n",
            "<Epoch 127> training loss: 2223.1469\n",
            "<Epoch 127> validation loss: 3060.6365\n",
            "<Epoch 128> training loss: 2231.6943\n",
            "<Epoch 128> validation loss: 2734.5318\n",
            "<Epoch 129> training loss: 2230.0292\n",
            "<Epoch 129> validation loss: 3036.374\n",
            "<Epoch 130> training loss: 2225.8612\n",
            "<Epoch 130> validation loss: 2521.79\n",
            "<Epoch 131> training loss: 2219.3821\n",
            "<Epoch 131> validation loss: 3798.219\n",
            "<Epoch 132> training loss: 2220.5304\n",
            "<Epoch 132> validation loss: 2642.1769\n",
            "<Epoch 133> training loss: 2203.3031\n",
            "<Epoch 133> validation loss: 3313.8412\n",
            "<Epoch 134> training loss: 2216.1443\n",
            "<Epoch 134> validation loss: 3467.1211\n",
            "<Epoch 135> training loss: 2225.5545\n",
            "<Epoch 135> validation loss: 2870.5014\n",
            "<Epoch 136> training loss: 2212.6232\n",
            "<Epoch 136> validation loss: 2930.5024\n",
            "<Epoch 137> training loss: 2219.2248\n",
            "<Epoch 137> validation loss: 2687.8948\n",
            "<Epoch 138> training loss: 2210.0542\n",
            "<Epoch 138> validation loss: 3050.2054\n",
            "<Epoch 139> training loss: 2205.308\n",
            "<Epoch 139> validation loss: 3275.6058\n",
            "<Epoch 140> training loss: 2202.4005\n",
            "<Epoch 140> validation loss: 2478.446\n",
            "<Epoch 141> training loss: 2194.8331\n",
            "<Epoch 141> validation loss: 2971.6542\n",
            "<Epoch 142> training loss: 2204.1885\n",
            "<Epoch 142> validation loss: 3015.4778\n",
            "<Epoch 143> training loss: 2197.0164\n",
            "<Epoch 143> validation loss: 2599.4852\n",
            "<Epoch 144> training loss: 2206.1386\n",
            "<Epoch 144> validation loss: 2361.8441\n",
            "<Epoch 145> training loss: 2196.828\n",
            "<Epoch 145> validation loss: 3013.6037\n",
            "<Epoch 146> training loss: 2192.6306\n",
            "<Epoch 146> validation loss: 2514.2813\n",
            "<Epoch 147> training loss: 2212.7588\n",
            "<Epoch 147> validation loss: 2654.0088\n",
            "<Epoch 148> training loss: 2204.2544\n",
            "<Epoch 148> validation loss: 2625.0116\n",
            "<Epoch 149> training loss: 2197.6472\n",
            "<Epoch 149> validation loss: 2879.6894\n",
            "<Epoch 150> training loss: 2199.0965\n",
            "<Epoch 150> validation loss: 2335.6217\n",
            "Model saved\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd62ab14bb0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model = model()\n",
        "model.load_state_dict(torch.load('./model.ckpt'))\n",
        "\n",
        "epochs = 1\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "DATA1, LABEL1 = generate_data(100)\n",
        "\n",
        "test_dataset = data_set(DATA1, LABEL1)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "average_loss = 0\n",
        "predicts = []\n",
        "labels = []\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (batch_x, batch_y) in enumerate(test_dataloader):\n",
        "            pred_y = model(batch_x)\n",
        "            predicts.append(pred_y[0])\n",
        "            labels.append(batch_y[0])\n",
        "            loss = criterion(pred_y, batch_y)\n",
        "            print(f\"{idx+1}  y:{batch_y[0]}  pred_y:{pred_y[0]}  loss: {criterion(batch_y[0], pred_y[0])}\")\n",
        "            \n",
        "            total_loss += float(loss) / len(test_dataloader)\n",
        "        average_loss += total_loss/epochs\n",
        "\n",
        "print(f\"Average Loss: {average_loss}\")\n",
        "plt.title(\"Model Test\")\n",
        "plt.scatter([i for i in range(1, len(predicts)+1)], predicts, color ='red')\n",
        "plt.scatter([i for i in range(1, len(labels)+1)], labels, color ='blue')\n",
        "plt.ylabel(\"Value\")\n",
        "plt.xlabel(\"n th data\")\n",
        "plt.grid(True)\n",
        "plt.show()\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "HbaU5AUG1-aC",
        "outputId": "b6b8baab-b206-46a4-ed66-9d4a5089a41e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmodel = model()\\nmodel.load_state_dict(torch.load(\\'./model.ckpt\\'))\\n\\nepochs = 1\\ncriterion = nn.MSELoss()\\n\\nDATA1, LABEL1 = generate_data(100)\\n\\ntest_dataset = data_set(DATA1, LABEL1)\\n\\ntest_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=True)\\n\\naverage_loss = 0\\npredicts = []\\nlabels = []\\nfor epoch in range(epochs):\\n    total_loss = 0\\n    with torch.no_grad():\\n        for idx, (batch_x, batch_y) in enumerate(test_dataloader):\\n            pred_y = model(batch_x)\\n            predicts.append(pred_y[0])\\n            labels.append(batch_y[0])\\n            loss = criterion(pred_y, batch_y)\\n            print(f\"{idx+1}  y:{batch_y[0]}  pred_y:{pred_y[0]}  loss: {criterion(batch_y[0], pred_y[0])}\")\\n            \\n            total_loss += float(loss) / len(test_dataloader)\\n        average_loss += total_loss/epochs\\n\\nprint(f\"Average Loss: {average_loss}\")\\nplt.title(\"Model Test\")\\nplt.scatter([i for i in range(1, len(predicts)+1)], predicts, color =\\'red\\')\\nplt.scatter([i for i in range(1, len(labels)+1)], labels, color =\\'blue\\')\\nplt.ylabel(\"Value\")\\nplt.xlabel(\"n th data\")\\nplt.grid(True)\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}